openhpc_packages_extra_nrel: # extra indirection to allow lab environment to filter this
  - slurm-libpmi-ohpc
  - bzip2
  - curl
  - fio
  - libaio
  - libaio-devel
  - git
  - gzip
  - make
  - patch
  - python3
  - tar
  - unzip
  - wget
  - xz
  - zstd
  - tmux
  - vim
  - slurm-libpmi-ohpc
  - slurm-sview-ohpc
  - gnu9-compilers-ohpc
  - nhc-ohpc
  - lmod-ohpc
  - ohpc-autotools
  - ohpc-base
  - ohpc-base-compute
  - openblas-gnu9-ohpc
  - ohpc-gnu9-openmpi4-perf-tools
  - mpich-ofi-gnu9-ohpc
  - mpich-ucx-gnu9-ohpc
  - mvapich2-gnu9-ohpc
  - openmpi4-gnu9-ohpc
  - ucx-ohpc
  - hdf5-gnu9-ohpc
  - netcdf-gnu9-impi-ohpc
  - netcdf-gnu9-mpich-ohpc
  - netcdf-gnu9-mvapich2-ohpc
  - netcdf-gnu9-openmpi4-ohpc
  - phdf5-gnu9-impi-ohpc
  - phdf5-gnu9-mpich-ohpc
  - phdf5-gnu9-mvapich2-ohpc
  - phdf5-gnu9-openmpi4-ohpc
  - pnetcdf-gnu9-impi-ohpc
  - pnetcdf-gnu9-mpich-ohpc
  - pnetcdf-gnu9-mvapich2-ohpc
  - pnetcdf-gnu9-openmpi4-ohpc
  - charliecloud-ohpc
  - singularity-ohpc
  - boost-gnu9-impi-ohpc
  - boost-gnu9-mpich-ohpc
  - boost-gnu9-mvapich2-ohpc
  - boost-gnu9-openmpi4-ohpc

# Additional parameters to set in slurm.conf - use yaml format
openhpc_config_extra:
  LaunchParameters: use_interactive_step
  FirstJobId: '50000000'
  PropagateResourceLimits: 'NONE'
  # vs cgid
  # ProctrackType: proctrack/cgroup
  ReturnToService: '1'
  SlurmctldPidFile: /var/run/slurmctld.pid
  SlurmdPidFile: /var/run/slurmd.pid
  SlurmdSpoolDir: /var/spool/slurm/slurmd
  StateSaveLocation: /var/spool/slurm/slurmctld
  SwitchType: 'switch/none'
  TaskPlugin: 'task/affinity'
  # TaskPlugin: 'task/affinity,task/cgroup'

  # Use sbatch or srun, however...
  # Don't recomend salloc, but this makes it less troublesome
  #### This fails kbendl - 2021-07-05
  # SallocDefaultCommand: 'srun  --pty --gres=NONE --preserve-env $SHELL'

  # PROLOGUE & EPILOGUE
  # JobSubmitPlugins: 'require_timelimit,lua'
  # PrologSlurmctld: '/nopt/slurm/etc/prologslurmctld.sh'
  # Prolog: '/nopt/slurm/etc/prolog.d/*'
  # PrologFlags: 'X11'
  # X11Parameters: 'local_xauthority'
  # Epilog: '/nopt/slurm/etc/epilog.d/*'
  # PrologEpilogTimeout: 180
  # UnkillableStepTimeout: 180

  # FairShare/FairTree Halflife decay, starting at 14 days on Oct 5th 2020:
  PriorityDecayHalfLife: '14-0'

  # PRIORITY
  # WHB on 2021-05-03 ref: HPCOPS-1447
  # to rebalance things to give a small (4%) weight to job age:
  #   fairshare 52%
  #   QoS 27%
  #   jobsize 12%
  #   partition = 5%
  #   Age 4%

  PriorityType: 'priority/multifactor'
  PriorityMaxAge: '14-0'
  PriorityWeightFairshare: '397659600'
  PriorityWeightAge: '30589200'
  PriorityWeightJobSize: '91767600'
  PriorityWeightQOS: '206477100'
  PriorityWeightPartition: '38236500'

  # TIMERS  size because db on spinning disk so had to increase
  BatchStartTimeout: '30'
  MessageTimeout: '100'
  TCPTimeout: '10'

  # SCHEDULING
  SchedulerType: 'sched/backfill'
  SelectType: 'select/cons_res'
  SelectTypeParameters: 'CR_Core'
  EnforcePartLimits: 'ALL'
  SchedulerParameters:
    - defer
    - default_queue_depth=10750
    - max_rpc_cnt=125
    - max_sched_time=6
    - partition_job_depth=10500
    - sched_max_job_start=200
    - sched_min_interval=2000000
    - batch_sched_delay=20
    - bf_max_job_test=13000
    - bf_interval=30
    - bf_continue
    - bf_window=15840
    - bf_resolution=250
    - max_switch_wait=172800
  #FastSchedule=1

# LOGGING
  SlurmctldDebug: 'info'
  SlurmctldLogFile: '/var/log/slurmctld.log'
  SlurmdDebug: 'info'

# ACCOUNTING
### start with a qos
  AccountingStorageEnforce:
    - safe
    - qos
    - limits
  AcctGatherNodeFreq: '30'
  AccountingStorageTRES: 'gres/gpu'

## added kmb
  # AcctGatherFilesystemType: 'acct_gather_filesystem/lustre'
  # JobAcctGatherType: 'jobacct_gather/linux'
  # JobAcctGatherParams: 'UsePss'
  # JobAcctGatherFrequency: 'task=30,energy=30'

# kbendl - the ephemeral mount for SSD?
  TmpFS: '/var/scratch'
  GresTypes: 'gpu'
  ### - MpiDefault: "pmi2"    ### get the launcher to work! pmi1, pmi2, or pmix
  # # spack mpi drivers were a mis-match... is this working now????

  # # NREL: see topology.conf
  # - 'TopologyPlugin: 'topology/tree'
  # - 'JobRequeue: 0
  # - 'MaxJobCount: 40000

  ### NREL:
  # Kurt... Need to build this - may need the mellanox info here.
  #NHC Node Health Check
  # - HealthCheckProgram: '/usr/sbin/nhc'
  # - HealthCheckInterval: 300
  # - HealthCheckNodeState: 'CYCLE,ANY'
