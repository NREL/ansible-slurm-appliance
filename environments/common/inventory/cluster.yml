# Yaml-format ansible inventory with default hostvars. It should be overriden/extended by an environment's own cluster.yml file.
# NB: This is the lowest-possible priority - it will be overridden by inventory files in other inventories, 'all' group_vars, etc.
# Host and group vars defined here are available even when no hosts have been provisioned (e.g. during provisioning or image build).

all:
  vars:
    # REQUIRED variables - to be defined in an environment:
    cluster_key_pair: # YES
    cluster_network_name: # NB: either cluster_network_name OR node_interfaces must be defined
    # Either node_image_name or node_image_id must be defined:
    #node_image_name:
    #node_image_id:
    # Either node_flavor_name or node_flavor_id must be defined:
    #node_flavor_name:
    #node_flavor_id:

    # OPTIONAL variables:
    cluster_name: "{{ lookup('env', 'APPLIANCES_ENVIRONMENT_NAME') }}"
    cluster_tld: invalid
    node_fqdn: "{{ inventory_hostname }}.{{ cluster_name }}.{{ cluster_tld }}" # can't be in role, needed elsewhere
    cluster_security_groups:  # not required in ansible
      - name: secgroup-cluster
        description: "Rules for the whole Slurm cluster"
        rules:
          # Allow all egress for all cluster nodes
          - direction: egress
          # Allow all ingress between cluster nodes
          - direction: ingress
            remote_group: "secgroup-cluster"
      - name: secgroup-login
        description: "Rules for login nodes"
        rules:
          # Allow SSH from anywhere to login nodes
          - direction: ingress
            protocol: tcp
            port: 22
          # Allow HTTPS from anywhere to login node
          - direction: ingress
            protocol: tcp
            port: 443

    node_interfaces: # not sure we want this in ansible?? except we need to define defaults here
      - network_name: "{{ cluster_network_name }}"
        security_groups:
          - secgroup-cluster
    
    node_tf_ignore_changes: [] # terraform attributes to ignore for node changes
    # node_floating_ip_address: 
    
    # sizes of default control volumes - see control hostvars:
    state_volume_size: 150 # GB
    home_volume_size: 100 # GB
    
    # this default works when there are no child groups in the 'compute' group
    # and no partition customisation is required
    openhpc_slurm_partitions:
      - name: "compute"
    
    node_user_data: | # don't need this from ansible, could move to role? it might grow other role-depedent bits though ...
      #cloud-config
      {% if cluster_ssh_keys is defined %}
      {% for ssh_key in cluster_ssh_keys %}
        - {{ ssh_key }}
      {% endfor %}
      {% endif %}
      fs_setup:
      {% for volume in node_volumes %}
        - label: {{ volume.label }}
          filesystem: ext4
      {# e.g. first volume mounted by default at /dev/vdb #}
          device: {{ node_volume_device_prefix | default('/dev/vd')}}{{ 'abcdefg'[loop.index] }}
          partition: auto
      {% endfor %}
      mounts:
      {% for volume in node_volumes %}
        - [LABEL={{ volume.label }}, {{ volume.mount_point }}, auto, "{{ volume.mount_options | default('') }}"]
      {% endfor %}

control:
  vars:
    appliances_state_dir: /var/lib/state
    node_volumes:
      # Default control node volume configuration:
      - label: state
        description: State for control node
        size: "{{ state_volume_size }}"
        mount_point: "{{ appliances_state_dir }}"
      - label: home
        description: Home for cluster
        size: "{{ home_volume_size }}"
        mount_point: /exports/home
        mount_options: 'x-systemd.required-by=nfs-server.service,x-systemd.before=nfs-server.service'

login:
  vars:
    node_interfaces:
      - network_name: "{{ cluster_network_name }}"
        security_groups:
          - secgroup-cluster
          - secgroup-login
