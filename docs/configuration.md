# Configuration

## Environments

The Appliance configuration is provided by one or more "environments", contained in the `environments/` directory. A "concrete" environment defines a particular instantiation of the Appliance, such as "dev" or "prod" clusters. Multiple concrete environments may be defined in the repository, with one "activated" to select the cluster to deploy or modify. Environments can inherit configuration from parent environments. This allows for shared configuration, e.g. both "dev" and "prod" environments could inherit shared configuration from a "site" environment.

Three environment directories exist in the repository by default:

- `common` defines the default Appliance default configuration. This is not a concrete environment and cannot be activated.
- `skeleton` is not a real environment but can be used to create a new environment as described in the quickstart:
- `arcus` is a concrete environment (produced using the cookiecutter skeleton) for CI. It can be deleted in clones of the repository although it may be useful as a reference.

As created by the cookiecutter skeleton, a concrete environment contains:
- An `activate` script which makes this the current environment (via environment variables).
- A `terraform/` directory with Terraform code to deploy infrastructure - this is provided as an example and starter only, and is likely to need adjustment for a specific cloud.
- An Ansible `inventory/` directory, described below.
- An `ansible.cfg` which uses inventory from the `common` environment, overriden by inventory from this environment:

        ```ini
        [defaults]
        inventory = ../common/inventory,inventory

    Additional inventory paths could be added here for shared site-specific configuration etc.

Concrete environments may also contain:
- Additional playbooks in `/hooks` to run before or after the default tasks.
- Environment-specific files.

Inherited environments are the same except they do not contain the `activate` script or an `ansible.cfg`.

In general, the defaults provided by the `common` environment plus the specific configuration generated by the cookiecutter skeleton are sufficent to produce a functioning Slurm Appliance.

## Default Terraform
The cookiecutter-generated Terraform is provided as a demo, working example and for CI use. It is likely that a production deployment will need to modify this for example to add floating IPs, different network arrangements or different resource lifecycles. However the cookiecutter Terraform provides variables to:
- Modify the number and type of nodes
- Modify the size of persistent directories
- Set security groups to permit access from other networks
- Use different VNIC types (e.g. for RDMA-capable networks)

See `environments/<environment>/terraform/variables.tf` (or the template [here](../environments/skeleton/\{\{cookiecutter.environment\}\}/terraform/variables.tf)) for details. Terraform variable values should be set in `environments/<environment>/terraform/terraform.tfvars`.

Note that Terraform is not a hard requirement of the Appliance. It could be replaced by another provisioning system (e.g. HEAT on OpenStack) or omitted entirely to deploy the appliance on "unmanaged" hosts. However the cookiecutter Terraform completes the Appliance configuration by templating some Ansible inventory variables [described below](#required-hosts-groups-and-variables), and this would have to be replicated by any alternative.

## Ansible Inventory Structure
The [Ansible inventory](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html) for an environment is in `environments/<environment>/inventory/`. This defines hosts, host variables and group variables. Generally it should contain the following:
- A `hosts` file. This defines the hosts in the appliance. Generally it should be templated out by the deployment automation (as in the cookiecutter skeleton Terraform) so it is also a convenient place to define variables which depend on the deployed hosts such as connection variables, IP addresses, ssh proxy arguments etc. 
- A `groups` file defining Ansible groups. This essentially controls which features of the appliance are enabled and on which nodes they are deployed. The `common/inventory/groups` file defines some required default groups but must be supplemented by a groups file in the concrete environment. Cookiecutter-created environments get a `groups` file copied from `environments/common/layouts/everything`. As its name suggests, this template deploys essentially all functionality (except for functionality which may only be needed on specific clouds).
- A `group_vars/` directory defining variables for the Ansible inventory groups. This repository generally follows a convention where functionality is defined using Ansible roles, with the roles applied to a group of the same name, e.g. the `openhpc` role (for Slurm) runs on hosts in the `openhpc` group and associated variables are prefixed `openhpc_`. The meaning and use of each group is described in comments in `environments/common/inventory/groups`. Default values for each group/functionality are defined in `environments/common/inventory/group_vars/all/<group_name>.yml` [^1] and may be overriden using e.g.; `environments/<environment>/inventory/group_vars/<group_name>/overrides.yml` or similar. General values not associated with a specific group/functionality are defined in `environments/common/inventory/group_vars/all/defaults.yml` and could be overriden if necessary using e.g. `environments/<environment>/inventory/group_vars/all/overrides.yml`.

[^1] The `all` group is used for the common environment to ensure it may always be overriden.

## Required hosts, groups and variables

This section defines the configuration which must be provided by a concrete environment, i.e. is missing from the `common` enviroment. The cookiecutter Terraform automatically generates most of these, as shown in [brackets] below.

- Groups `control`, `login` and `compute` containing the appropriate hosts [templated into `<environment>/inventory/hosts`]. If multiple subsets of compute nodes are required (e.g. `small`, `himem`, `gpu`) those should be defined as child groups of `compute`.
- The cluster name must be set for all hosts using `openhpc_cluster_name` [templated into `<environment>/inventory/hosts`, section `[all:vars]`].
- Various secrets, generated by the `ansible/adhoc/generate-passwords.yml` playbook into `<environment>/inventory/group_vars/all/secrets.yml`.
- Each Slurm partition must have:
    - An inventory group `<cluster_name>_<partition_name>` defining the hosts it contains - these must be homogenous in terms of CPU and memory.
    - An entry in the `openhpc_slurm_partitions` mapping for `openhpc` hosts.
  [templated into `<environment>/inventory/group_vars/all/partitions.yml`]
- OpenOndemand is enabled by default for login nodes and requires the following variables described in [ansible/roles/openondemand](../ansible/roles/openondemand/README.md) - **NB** these are NOT set by the cookiecutter Terraform.
    - `openondemand_auth`
    - `openondemand_host_regex`
    - `openondemand_jupyter_partition`
    - `openondemand_desktop_partition`
    - `openondemand_servername`

## Optional functionality
The following functionality is not enabled by the default ("everything template") cookiecutter environment. Enable them by adding hosts to the relevant group in e.g. `<environment>/groups` and setting required variables:
- `rebuild`: On an OpenStack cloud, rebuilding/reimaging compute nodes from Slurm can be enabled by defining a `rebuild` group containing `control` and `compute` or a subset. See the [rebuild role](https://github.com/stackhpc/ansible_collection_slurm_openstack_tools/blob/main/roles/rebuild/README.md) for variables and usage.
- `basic_users`: Basic user management can be enabled via Ansible templating by defining a `basic_users` group containing the `openhpc` group. See the [basic_users](../ansible/roles/basic_users/README.md) role for variables.

## Common configuration changes
This section describes some configuration changes which are frequently required. Review the appliance defaults in `common/inventory/group_vars/all/<group_name>` before adding overrides.

- Additional slurm configuration can be provided using [openhpc role variables](https://github.com/stackhpc/ansible-role-openhpc#role-variables). Note the Appliance [provides](../environments/common/inventory/group_vars/all/openhpc.yml) `openhpc_packages_extra` and `openhpc_config_extra` to allow an environment to add configuration without overriding the Appliance defaults.

- Passwordless login to Grafana (for view only) can be enabled by setting `grafana_auth_anonymous: true` - understand the [implications](https://grafana.com/docs/grafana/latest/administration/security/#implications-of-enabling-anonymous-access-to-dashboards) before setting this.
